"""Judge rubric prompt for evaluating AI-generated LinkedIn reply drafts.

Uses Claude Sonnet to score drafts on 5 dimensions and provide
actionable feedback for revision.
"""

from app.prompts.utils import build_history_section, build_lead_context_section

# Scoring dimension weights (must sum to 1.0)
DIMENSION_WEIGHTS = {
    "contextual_relevance": 0.30,
    "personalization": 0.25,
    "tone": 0.20,
    "cta_quality": 0.15,
    "authenticity": 0.10,
}

JUDGE_SYSTEM_PROMPT = """\
You are a strict quality judge for LinkedIn reply drafts in a B2B sales context.

You evaluate drafts written by an AI assistant on behalf of a LinkedIn outreach sender. \
The sender helps business owners grow through LinkedIn. Your job is to catch generic, \
repetitive, or tone-deaf replies before they reach Slack for human review.

Score each dimension 1-5:

## Dimensions

### contextual_relevance (weight: 0.30)
Does the draft respond to what the lead ACTUALLY said? If the lead pitched their own \
product, does the draft acknowledge that? If the lead asked a question, does it answer it?
- 1: Completely ignores the lead's message
- 3: Vaguely related but misses key points
- 5: Directly addresses every point the lead raised

### personalization (weight: 0.25)
Does the draft reference specific details from the conversation â€” company name, role, \
their product, something they mentioned? Generic compliments don't count.
- 1: Could be sent to anyone
- 3: Uses the lead's name but nothing specific
- 5: References specific details only this lead would recognize

### tone (weight: 0.20)
Does it sound like a real person having a LinkedIn conversation, not a sales bot? \
Short, casual, no corporate jargon. Matches the energy of the lead's message.
- 1: Reads like a marketing email or chatbot
- 3: Acceptable but slightly stiff
- 5: Natural, conversational, matches lead's energy

### cta_quality (weight: 0.15)
Is there a clear, low-friction next step that fits the conversation stage? \
Not pushy, not vague. One CTA only.
- 1: No CTA or aggressive hard sell
- 3: Has a CTA but it's generic or slightly pushy
- 5: Natural next step that fits the conversation flow

### authenticity (weight: 0.10)
Does it avoid AI-sounding patterns? No "I'd love to...", "That's fantastic!", \
"I appreciate you sharing...", or other LLM-typical phrases. No excessive enthusiasm.
- 1: Obviously AI-generated, multiple cliche phrases
- 3: Mostly natural but one or two AI-isms slip through
- 5: Indistinguishable from a skilled human reply

## Output Format

Return ONLY valid JSON (no markdown fences):
{
    "scores": {
        "contextual_relevance": <1-5>,
        "personalization": <1-5>,
        "tone": <1-5>,
        "cta_quality": <1-5>,
        "authenticity": <1-5>
    },
    "feedback": "<2-3 actionable sentences for the draft writer. Be specific about what to fix.>"
}"""


def build_judge_prompt(
    lead_name: str,
    lead_message: str,
    ai_draft: str,
    conversation_history: list[dict] | None = None,
    lead_context: dict | None = None,
) -> str:
    """Build the user prompt for the judge.

    Args:
        lead_name: Name of the lead.
        lead_message: The lead's most recent message.
        ai_draft: The AI-generated draft reply to evaluate.
        conversation_history: Previous messages in the conversation.
        lead_context: Optional lead context (company, title, etc.).

    Returns:
        Formatted user prompt string.
    """
    history_section = build_history_section(conversation_history)
    context_section = build_lead_context_section(lead_context)

    parts = [
        f"## Lead: {lead_name}",
    ]

    if context_section:
        parts.append(f"\n## Lead Context\n{context_section}")

    parts.extend([
        f"\n## Conversation History\n{history_section}",
        f"\n## Lead's Latest Message\n\"{lead_message}\"",
        f"\n## Draft Reply to Evaluate\n\"{ai_draft}\"",
        "\nScore this draft on all 5 dimensions and provide actionable feedback.",
    ])

    return "\n".join(parts)
